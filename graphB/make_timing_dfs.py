import numpy as np
import pandas as pd
from sys import argv
from datetime import datetime, timedelta
import os

### This program makes a timing dataframe from output logfiles generated by graphB.
### It can take multiple files as command line arguments manually, in which it will generate
### one dataframe with the results of each log as its own row.
### This file is run automatically in the postprocessing step of graphB. See README in
### graphB for description of the different timing metrics generated.


def convert_timedelta(convert_dict):
    new_dict = {}
    for key in convert_dict:
        if key != "FILENAME":
            try:
                for val in convert_dict[key]:
                    if val != "None":
                        time = val.split(":")
                        if len(list(time[0])) > 1:
                            d = int(time[0].split("day,")[0])
                            h = (24 * d) + int(time[0].split("day,")[1])
                        else:
                            h = int(time[0])
                        m = int(time[1])
                        if len(time[2].split(".")) > 1:
                            s = int(time[2].split(".")[0])
                            ms = int(time[2].split(".")[1])
                        else:
                            s = int(time[2])
                            ms = 0
                        val = timedelta(hours=h, minutes=m, seconds=s, microseconds=ms)
                        new_dict.setdefault(key, []).append(val)
                    else:
                        val = timedelta(hours=0, minutes=0, seconds=0, microseconds=0)
                        new_dict.setdefault(key, []).append(val)
            except Exception as error:
                print(
                    "ERROR IN CONVERT TIMEDELTA FUNCTION, key is: ",
                    key,
                    " File is: ",
                    convert_dict["FILENAME"],
                    " Exception: ",
                    error,
                )
    return new_dict


def create_avg_or_sum_labels(avg_series, sum_series, new_series):
    if not avg_series.empty:
        for index_to_change in avg_series.index:
            if index_to_change != "FILENAME":
                new_series["AVG_" + index_to_change] = avg_series[index_to_change]
                new_series["SUM_" + index_to_change] = sum_series[index_to_change]
    else:
        keywords = [
            "TOTAL_BALANCE_TIME",
            "BALANCE_TIME",
            "COMPONENT_LIST_GEN_TIME",
            "COMPONENT_STATS_TIME",
            "TREE_TIME",
        ]
        for word in keywords:
            new_series["AVG_" + word] = timedelta(
                hours=0, minutes=0, seconds=0, microseconds=0
            )
            new_series["SUM_" + word] = timedelta(
                hours=0, minutes=0, seconds=0, microseconds=0
            )
    return new_series


def change_to_seconds(timedelta_series):
    timedelta_series = timedelta_series.total_seconds()
    return timedelta_series


def create_write_filename(outfiles):
    outfile = os.path.normpath(outfiles[0])
    split_dir = os.path.dirname(outfile).split(os.sep)
    write_dir = (
        os.sep.join(split_dir[:-2]) + "/Output_Data/Timing/" + split_dir[-1] + "/"
    )
    os.makedirs(write_dir, exist_ok=True)
    write_file = (
        write_dir
        + "_".join(os.path.basename(outfile).split("_")[0:3])
        + "_timing_results"
    )
    return write_file


def create_timing_results(output_files, write_filename):
    FINAL_COLUMNS = [
        "AVG_COMPONENT_LIST_GEN_TIME",
        "AVG_COMPONENT_STATS_TIME",
        "SUM_COMPONENT_STATS_TIME",
        "SUM_COMPONENT_LIST_GEN_TIME",
        "SUM_TREE_TIME",
        "AVG_TREE_TIME",
        "AVG_TOTAL_BALANCE_TIME",
        "AVG_BALANCE_TIME",
        "SUM_TOTAL_BALANCE_TIME",
        "SUM_BALANCE_TIME",
        "GLOBAL_TIME",
        "VERTEX_DF_TIME",
        "MATRIX_CREATE_TIME",
        "SYM_MATRIX_CREATE_TIME",
        "CALC_STATUS_TIME",
        "TOTAL_PREPROCESS_TIME",
        "TOTAL_PROCESS_TIME",
        "TOTAL_POSTPROCESS_TIME",
        "COMPUTE_TIME_NO_IO",
    ]

    total_df_datetime = pd.DataFrame(columns=FINAL_COLUMNS)
    total_df_seconds = pd.DataFrame(columns=FINAL_COLUMNS)

    for outfile in output_files:
        outfile_source = os.path.basename(outfile).split("_")[2]
        tree_keywords = {
            "COMPONENT_LIST_GEN_TIME": [],
            "COMPONENT_STATS_TIME": [],
            "TREE_TIME": [],
            "BALANCE_TIME": [],
            "TOTAL_BALANCE_TIME": [],
            "FILENAME": "",
        }
        global_keywords = {
            "TOTAL_PREPROCESS_TIME": [],
            "TOTAL_PROCESS_TIME": [],
            "TOTAL_POSTPROCESS_TIME": [],
            "GLOBAL_TIME": [],
            "VERTEX_DF_TIME": [],
            "CALC_STATUS_TIME": [],
            "MATRIX_CREATE_TIME": [],
            "SYM_MATRIX_CREATE_TIME": [],
            "FILENAME": "",
        }

        with open(outfile, "r") as outfile:
            global_keywords["FILENAME"] = outfile
            tree_keywords["FILENAME"] = outfile
            for line in outfile:
                if outfile_source == "LEAP":
                    keyword = line.split(":")[0]
                elif outfile_source == "current":
                    keyword = line.split(":")[2]
                if keyword in tree_keywords:
                    tree_keywords.setdefault(keyword, []).append(
                        line.split(")")[1].replace("\n", "").replace(" ", "")
                    )
                if keyword in global_keywords:
                    if not global_keywords[
                        keyword
                    ]:  # only want one entry in case there were multiple input h5s created.
                        global_keywords[keyword].append(
                            line.split(")")[1].replace("\n", "").replace(" ", "")
                        )

        tree_keywords = convert_timedelta(tree_keywords)
        global_keywords = convert_timedelta(global_keywords)
        global_keywords["GLOBAL_TIME"] = (
            global_keywords["TOTAL_PREPROCESS_TIME"][0]
            + global_keywords["TOTAL_PROCESS_TIME"][0]
            + global_keywords["TOTAL_POSTPROCESS_TIME"][0]
        )

        ### These two for loops put in because spark doesn't consistently write all the print output.
        ### This resulted in the tree time having one less entry than the other times and the mean on
        ### line 55 would not compute. Solution was to compute the mean of all the other entries and
        ### add in another entry equal to the mean for that column so the length of all columns would
        ### match while still not affecting the overall average.
        max_length = 0
        for key in tree_keywords:
            if len(tree_keywords[key]) > max_length:
                max_length = len(tree_keywords[key])
        for key in tree_keywords:
            mean = sum(tree_keywords[key], timedelta()) / len(tree_keywords[key])
            if len(tree_keywords[key]) < max_length:
                tree_keywords.setdefault(key, []).append(mean)

        tree_sums = pd.DataFrame(tree_keywords).sum()
        tree_series = pd.DataFrame(tree_keywords).mean()
        global_series = pd.DataFrame(global_keywords).mean()
        total_series = tree_series.append(global_series)
        ### divide the info into average and sums
        total_series = create_avg_or_sum_labels(tree_series, tree_sums, total_series)

        ### create a second version of everything in seconds
        total_series_seconds = pd.Series()

        FINAL_COLUMN_ORDER = [
            "GLOBAL_TIME",
            "TOTAL_PREPROCESS_TIME",
            "TOTAL_PROCESS_TIME",
            "TOTAL_POSTPROCESS_TIME",
            "SUM_TOTAL_BALANCE_TIME",
            "AVG_TOTAL_BALANCE_TIME",
            "SUM_BALANCE_TIME",
            "AVG_BALANCE_TIME",
            "SUM_TREE_TIME",
            "AVG_TREE_TIME",
            "SUM_COMPONENT_LIST_GEN_TIME",
            "AVG_COMPONENT_LIST_GEN_TIME",
            "SUM_COMPONENT_STATS_TIME",
            "AVG_COMPONENT_STATS_TIME",
            "VERTEX_DF_TIME",
            "CALC_STATUS_TIME",
            "MATRIX_CREATE_TIME",
            "SYM_MATRIX_CREATE_TIME",
        ]
        for name in FINAL_COLUMN_ORDER:
            if name not in total_series.index:
                total_series[name] = timedelta(
                    hours=0, minutes=0, seconds=0, microseconds=0
                )
        for column in total_series.index:
            column_in_seconds = change_to_seconds(total_series[column])
            total_series_seconds[column] = column_in_seconds

        current_df_datetime = pd.DataFrame(
            [total_series], index=[str(outfile).split("=")[1].split(" ")[0]]
        )
        current_df_seconds = pd.DataFrame(
            [total_series_seconds], index=[str(outfile).split("=")[1].split(" ")[0]]
        )
        total_df_datetime = total_df_datetime.append(current_df_datetime, sort=True)
        total_df_seconds = total_df_seconds.append(current_df_seconds, sort=True)
        tree_keywords = tree_keywords.clear()
        global_keywords = global_keywords.clear()

    total_df_datetime = total_df_datetime[FINAL_COLUMN_ORDER]
    total_df_seconds = total_df_seconds[FINAL_COLUMN_ORDER]

    if write_filename is None:
        total_df_datetime.to_csv("timing_results_datetime.csv", encoding="utf-8")
        total_df_seconds.to_csv("timing_results_seconds.csv", encoding="utf-8")
    else:
        total_df_datetime.to_csv(write_filename + "_datetime.csv", encoding="utf-8")
        total_df_seconds.to_csv(write_filename + "_seconds.csv", encoding="utf-8")

    print("wrote timing results csvs")


if __name__ == "__main__":

    outfiles = argv[1:]
    write_file = None

    if len(outfiles) == 1:
        write_file = create_write_filename(outfiles)

    create_timing_results(outfiles, write_file)
